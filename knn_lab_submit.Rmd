---
title: "KNN Lab"
author: "Helena Lindsay"
date: "10/25/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE,message=FALSE,warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
```

You left your job as a lobbyist because the political environment was become just too toxic to handle.  Luckily you landed a job in advertising! Unfortunately have a demanding and totally clueless boss. Clueless meaning that he doesn't understand data science, but he knows he wants it to be used to fix all the company's problems and you are just the data scientist to do it! 

Your company, Marketing Enterprises of Halifax or "MEH" is being beat out by the competition and wants a new way to determine the quality of its commercials. Your boss, Mr. Ed Rooney, would like the company's commercials to seem more like actual TV shows. So we wants you to develop a "machine learning thing" using the company’s internal data to classify when something is a commercial and when it is not. Mr. Rooney believes the company will then know how to trick potential future customers into thinking their commercials are actually still part of the show and as a result will pay more attention and thus buy more of the terrible products "MEH" is supporting (it's a terrible plan, but you have to make a living). 

Given that MEH is producing commercials more or less continuously you know there will be a need to update the model quite frequently, also being a newish data scientist and having a clueless boss you decide to use a accessible approach that you might be able to explain to Mr. Rooney, (given several months of dedicated one on one time), that approach is k-nearest neighbor. 

You'll also need to document your work extensively, because Mr. Rooney doesn't know he's clueless so he will ask lots of "insightful" questions and require lots of detail that he won't understand, so you'll need to have an easy to use reference document. Before you get started you hearken back to the excellent education you received at UVA and using this knowledge outline roughly 20 steps that need to be completed to build this algo for MEH and Ed, they are documented below...good luck. As always, the most important part is translating your work to actionable insights, so please make sure to be verbose in the explanation required for step 20.

As with the clustering lab, please be prepared to present a five minute overview of your findings. 
 


### 1: Load in the data, both the commercial dataset and the labels
```{r}
setwd("/Users/helenamaylindsay/Pictures/3rd_year_1st_semester/DS/lab7_KNN/rstudio-export-2")
commlabels = read_csv("cnn_commmercial_label.csv", col_names = FALSE)
commlabels = t(commlabels)

data = read_csv("tv_commercial_datasets_CNN_Cleaned.csv", col_names = commlabels)
data = data[-c(1),]

data$label = as.numeric(data$label)
```



```{r}
table(data$label)
baserate = table(data$label)[2] / sum(table(data$label))
baserate
```
There are 14,411 commercials and 8,134 non-commercials. At random, we have a 63.9% chance of correctly picking out a commercial.


### 2: Drop columns that contain different metrics for the same variable
```{r}
data2 <- data %>% select(-contains("var"))
head(data2)
```


### 3: Check to make sure that our variables are not highly correlated
```{r}
commercial_correlations = cor(data2)
head(commercial_correlations)
```

### 4: Determine which variables to remove
#### high correlations start around .7 or below -.7 I would especially remove variables that appear to be correlated with more than one variable. List your rationale here:
Motion distribution is highly correlated with frame differential distribution and motion distribution with values of 0.715 and -0.757, respectively. Short time energy is highly correlated with spectral flux with a value of 0.823. Spectral centroid is highly correlated with spectral roll off with a value of 0.809.


### 5: Subset the dataframe based on above
```{r}
subsetvars = c("shot_length", "zcr_mn", "fundamental_freq_mn", "label")
kdata = data2[subsetvars]
head(kdata)
```


### 6: Create an index that will divide the data into a 70/30 split
```{r,echo=TRUE}
set.seed(13)

kdata_train_rows = sample(1:nrow(kdata),
                          round(0.7 * nrow(kdata), 0),
                          replace = FALSE)
```


### 7: Use the index above to generate a train and test sets
```{r}
kdata_train = kdata[kdata_train_rows, ]
kdata_test = kdata[-kdata_train_rows, ]

nrow(kdata_train)
nrow(kdata_test)
```


## Train the classifier using k = 3
```{r}
# install.packages("class")
library(class)

set.seed(13)

kdata_3NN = knn(train = kdata_train[, c("shot_length", "zcr_mn", "fundamental_freq_mn"), drop = FALSE],
                test = kdata_test[, c("shot_length", "zcr_mn", "fundamental_freq_mn"), drop = FALSE],
                cl = kdata_train$label,
                k = 3,
                use.all = TRUE,
                prob = TRUE)
```



```{r}
str(kdata_3NN)
length(kdata_3NN)
```



```{r}
kdata_res = table(kdata_3NN, kdata_test$label)
kdata_res
```



```{r}
kdata_res[row(kdata_res) == col(kdata_res)]
```


### 8: Calculate the accuracy rate
```{r}
kNN_acc_com = sum(kdata_res[row(kdata_res) == col(kdata_res)]) / sum(kdata_res)
kNN_acc_com
kNN_acc_com - baserate
```
The accuracy rate was 70.299%. This was 6.378% better than the base rate.


### 9:  Run the confusion matrix function and comment on the model output
```{r,echo=FALSE}
library(caret)

confusionMatrix(as.factor(kdata_3NN), as.factor(kdata_test$label), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")
```
The confusion matrix tells use that our model has an accuracy of 70.3%. The model was also much better at correctly classifying commercials rather than non-commercials given our sensitivity and specificty rates of 80.76% and 51.8% respectively; this is based on the fact that the former refers to the true positive rate and the latter refers to the true negative rate. The average of these rates is our balanced accuracy which was reported to be 66.28%.


### 10: Run the "chooseK" function to find the perfect K
```{r}
chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k,                #<- number of neighbors considered
                  use.all = TRUE)       #<- control ties between class assignments
                                        #   If true, all distances equal to the kth largest are included
  conf_mat = table(class_knn, val_class)
  
  # Calculate the accuracy.
  accu = sum(conf_mat[row(conf_mat) == col(conf_mat)]) / sum(conf_mat)                         
  cbind(k = k, accuracy = accu)
}

kdata_diffk = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                                             train_set = kdata_train[, c("shot_length", "zcr_mn", "fundamental_freq_mn"), drop = FALSE],
                                             val_set = kdata_test[, c("shot_length", "zcr_mn", "fundamental_freq_mn"), drop = FALSE],
                                             train_class = kdata_train$label,
                                             val_class = kdata_test$label))
```


### 11: Create a dataframe so we can visualize the difference in accuracy based on K
```{r}
class(kdata_diffk)

kdata_diffk = data.frame(k = kdata_diffk[1,],
                             accuracy = kdata_diffk[2,])

kdata_diffk
```


### 12: Use ggplot to show the output and comment on the k to select
```{r}
library(ggplot2)

ggplot(kdata_diffk,
       aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3)
```


## Rerun the model  with "optimal" k 
```{r}
kdata_5NN = knn(train = kdata_train[, c("shot_length", "zcr_mn", "fundamental_freq_mn"), drop = FALSE],
                test = kdata_test[, c("shot_length", "zcr_mn", "fundamental_freq_mn"), drop = FALSE],
                cl = kdata_train$label,
                k = 5,
                use.all = TRUE,
                prob = TRUE)
```


### 13: Use the confusion matrix function to measure the quality of the new model
```{r}
confusionMatrix(as.factor(kdata_5NN), as.factor(kdata_test$label), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")
```


### 20: Summarize the differences in language Mr. Rooney may actually understand. Include a discussion on which approach k=3 or k="optimal" is the better method moving forward for "MEH". Most importantly draft comments about the over approach and model quality as it relates to addressing the problem proposed by Ed. 
The first classifier using k = 3 identified 3 of the input's nearest neighbors to determine whether the input were to be classified as commercial or non-commercial. The accuracy rate was 70.299%. This was 6.378% better than the base rate. The model was also much better at correctly classifying commercials rather than non-commercials given our sensitivity and specificity rates of 80.76% and 51.8% respectively; this is based on the fact that the former refers to the true positive rate and the latter refers to the true negative rate. The average of these rates is our balanced accuracy which was reported to be 66.28%.

As seen in step 16 & 17, when running the "chooseK" function to find the perfect K, we found that k=5 had an accuracy rate of 72.7%, higher than the classifier using k=3 (70.299%). The sensitivity rate and specificity rate were 84.19% and 50.37% respectively. This tells us that the model is better at correctly classifying commercials rather than non-commercials. The rate of correctly classifying commercials for the k=5 model was 84.19%, and was 3,43% higher than that of the k=3 classifier. On the other hand, the rate of correctly classifying non-commercials for the k=5 model was 50.37% and thus 1.43% lower than that of the k=3 model.  
However, when looking at the balanced accuracy rate for the K=5 model, which was reported to be 67.28%, we can see that it is 1.0% higher than the k=3 model. 

Since the goal of this project is to use the company’s internal data to classify when something is a commercial and when it is not, the model with a higher accuracy rate for classifying commercials is what is needed. Both the accuracy rate (71.97%) and balanced accuracy rate (67.28%) were higher in the k=5 model, compared to the k=3 model, and thus we should recommend the company use the k=5 model. 
The company should however keep in mind that the model would do a better job in classifying commercials over non-commercials.

